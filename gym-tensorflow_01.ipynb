{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-08 13:53:09,499] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_input = env.observation_space.shape[0]\n",
    "n_actions = int(re.findall('\\d',str(env.action_space))[0])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1.e-1\n",
    "training_epochs = 100\n",
    "n_steps = 100\n",
    "display_step = 100\n",
    "exp_a = 0.1\n",
    "exp_b = 0.0\n",
    "batch_size = 100\n",
    "loss_multiplier = 1\n",
    "max_memory = 20000\n",
    "discount = 0.9\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 4 # 1st layer number of features\n",
    "n_hidden_2 = 4 # 2nd layer number of features\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float64, [None, n_input])\n",
    "y = tf.placeholder(tf.float64, [None, n_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceQModel(object):\n",
    "    def __init__(self, max_memory=500, discount=.9):\n",
    "        # Memory replay parameters\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def exp_remember(self, states):\n",
    "        self.memory.append(states.copy())\n",
    "        if len(self.memory) > self.max_memory:\n",
    "          del self.memory[0]\n",
    "\n",
    "    # based on https://gist.github.com/EderSantana/c7222daa328f0e885093\n",
    "    def exp_get_batch(self,batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        n_examples = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((n_examples, n_input))\n",
    "        targets = np.zeros((n_examples, n_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,size=n_examples)):\n",
    "            #get_memory\n",
    "            states = self.memory[idx]\n",
    "            state_t = states['state_t']\n",
    "            state_tp1 = states['state_tp1']\n",
    "            action = states['action']\n",
    "\n",
    "            # input\n",
    "            inputs[i] = state_t.astype(np.float64)\n",
    "\n",
    "            # targets - not correcting those which are not taken\n",
    "            feed_dict = {x: states['state_t'].reshape(1,-1)}\n",
    "            targets[i] = sess.run(pred, feed_dict)\n",
    "            \n",
    "            # acted action\n",
    "            feed_dict = {x: states['state_tp1'].reshape(1,-1)}\n",
    "            Qsa = np.max(sess.run(pred, feed_dict))\n",
    "\n",
    "            # check if endgame and if not apply discount\n",
    "            if states['endgame']:\n",
    "                targets[i,action] = states['reward'] # assign just reward if endgame\n",
    "            else:\n",
    "                targets[i,action] = states['reward'] + self.discount * Qsa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],dtype=tf.float64)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],dtype=tf.float64)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_actions],dtype=tf.float64))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1],dtype=tf.float64)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2],dtype=tf.float64)),\n",
    "    'out': tf.Variable(tf.random_normal([n_actions],dtype=tf.float64))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_sum(tf.pow(pred-y, 2))/(2*batch_size)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: lost after 9, cost 0.0132851110676\n",
      "1: lost after 9, cost 0.0351267394777\n",
      "2: lost after 11, cost 0.029899303251\n",
      "3: lost after 9, cost 0.0197067634291\n",
      "4: lost after 9, cost 0.0743710002683\n",
      "5: lost after 31, cost 0.823964048087\n",
      "6: lost after 12, cost 0.39561808126\n",
      "7: lost after 9, cost 0.295080199084\n",
      "8: lost after 14, cost 0.512148328226\n",
      "9: lost after 48, cost 2.61133565385\n",
      "10: lost after 81, cost 1.91153745107\n",
      "11: lost after 68, cost 0.519286056907\n",
      "12: lost after 86, cost 0.401396074184\n",
      "13: lost after 63, cost 0.418735949306\n",
      "14: won!\n",
      "15: won!\n",
      "16: won!\n",
      "17: lost after 72, cost 0.215839674744\n",
      "18: won!\n",
      "19: won!\n",
      "20: won!\n",
      "21: won!\n",
      "22: won!\n",
      "23: won!\n",
      "24: lost after 98, cost 0.227931555383\n",
      "25: won!\n",
      "26: lost after 98, cost 0.244660035313\n",
      "27: lost after 87, cost 0.209337725435\n",
      "28: won!\n",
      "29: won!\n",
      "30: won!\n",
      "31: won!\n",
      "32: won!\n",
      "33: lost after 96, cost 0.195273969966\n",
      "34: won!\n",
      "35: won!\n",
      "36: won!\n",
      "37: won!\n",
      "38: lost after 40, cost 0.0869911708162\n",
      "39: won!\n",
      "40: won!\n",
      "41: won!\n",
      "42: won!\n",
      "43: won!\n",
      "44: won!\n",
      "45: won!\n",
      "46: lost after 50, cost 0.0950206336515\n",
      "47: won!\n",
      "48: won!\n",
      "49: won!\n",
      "50: lost after 56, cost 0.0996096425305\n",
      "51: lost after 12, cost 0.0366604073087\n",
      "52: won!\n",
      "53: won!\n",
      "54: won!\n",
      "55: won!\n",
      "56: won!\n",
      "57: won!\n",
      "58: won!\n",
      "59: won!\n",
      "60: won!\n",
      "61: lost after 67, cost 0.124611024095\n",
      "62: lost after 60, cost 0.0903819880235\n",
      "63: won!\n",
      "64: lost after 16, cost 0.0305349821184\n",
      "65: lost after 29, cost 0.0440172204327\n",
      "66: lost after 41, cost 0.121647950156\n",
      "67: lost after 34, cost 0.100674909955\n",
      "68: won!\n",
      "69: won!\n",
      "70: won!\n",
      "71: won!\n",
      "72: won!\n",
      "73: lost after 25, cost 0.0507346235754\n",
      "74: lost after 47, cost 0.0795123027361\n",
      "75: lost after 38, cost 0.0719714480633\n",
      "76: lost after 14, cost 0.0601900301109\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-c88c4cab87fa>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# get experience replay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Compute average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0630d53f1bab>\u001b[0m in \u001b[0;36mexp_get_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# targets - not correcting those which are not taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# acted action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    582\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    583\u001b[0m     \u001b[1;31m# Validate and process fetches.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 584\u001b[1;33m     \u001b[0mprocessed_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_process_fetches\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetches\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    585\u001b[0m     \u001b[0munique_fetches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    586\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocessed_fetches\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_process_fetches\u001b[1;34m(self, fetches)\u001b[0m\n\u001b[0;32m    524\u001b[0m           fetch_t = self.graph.as_graph_element(subfetch, allow_tensor=True,\n\u001b[0;32m    525\u001b[0m                                                 allow_operation=True)\n\u001b[1;32m--> 526\u001b[1;33m           \u001b[0mfetch_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_bytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m           \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOperation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_assert_fetchable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfetch_t\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/util/compat.pyc\u001b[0m in \u001b[0;36mas_bytes\u001b[1;34m(bytes_or_text)\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mIf\u001b[0m \u001b[1;33m`\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m`\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0ma\u001b[0m \u001b[0mbinary\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0municode\u001b[0m \u001b[0mstring\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m   \"\"\"\n\u001b[1;32m---> 39\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_type\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     40\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mbytes_or_text\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     41\u001b[0m   \u001b[1;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbytes_or_text\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbytes\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# initialize states and experience replay\n",
    "states = {}\n",
    "exp_replay = ExperienceQModel(max_memory=max_memory)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    state_tp1 = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        env.render()\n",
    "        state_t1 = np.array(state_tp1)\n",
    "        \n",
    "        # exploration cycle\n",
    "        if np.random.rand() <= exp_a-exp_b*epoch/100:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            feed_dict = {x: state_t1.reshape(1,-1)}\n",
    "            qvals = sess.run(pred, feed_dict)\n",
    "            action = np.argmax(qvals)\n",
    "\n",
    "        # take a next step\n",
    "        state_tp1, reward, done, info = env.step(action)\n",
    "\n",
    "        # print (reward,done)\n",
    "        # rewards\n",
    "        if (t == 99) and (done == False):\n",
    "            print(\"{}: won!\".format(epoch))\n",
    "        if done:\n",
    "            reward = 0;\n",
    "\n",
    "        # store experience\n",
    "        states['action'] = action\n",
    "        states['reward'] = float(reward)\n",
    "        states['endgame'] = done\n",
    "        states['state_t'] = np.array(state_t1)\n",
    "        states['state_tp1'] = np.array(state_tp1)\n",
    "        exp_replay.exp_remember(states)\n",
    "\n",
    "        # get experience replay\n",
    "        x_batch, y_batch = exp_replay.exp_get_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: x_batch, y: y_batch})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / n_steps\n",
    "\n",
    "        # Lost\n",
    "        if done:\n",
    "            print(\"{}: lost after {}, cost {}\".format(epoch,t+1,avg_cost))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_replay.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict = {x: states['state_tp1'].reshape(1,-1)}\n",
    "qvals = sess.run(pred, feed_dict)\n",
    "print(qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
