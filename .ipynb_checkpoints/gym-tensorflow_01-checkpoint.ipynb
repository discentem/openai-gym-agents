{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import gym\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-08 13:50:20,440] Making new env: CartPole-v0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "n_input = env.observation_space.shape[0]\n",
    "n_actions = int(re.findall('\\d',str(env.action_space))[0])\n",
    "\n",
    "# Parameters\n",
    "learning_rate = 1.e-1\n",
    "training_epochs = 100\n",
    "n_steps = 100\n",
    "display_step = 100\n",
    "exp_a = 0.1\n",
    "exp_b = 0.0\n",
    "batch_size = 100\n",
    "loss_multiplier = 1\n",
    "max_memory = 20000\n",
    "discount = 0.9\n",
    "\n",
    "# Network Parameters\n",
    "n_hidden_1 = 4 # 1st layer number of features\n",
    "n_hidden_2 = 4 # 2nd layer number of features\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float64, [None, n_input])\n",
    "y = tf.placeholder(tf.float64, [None, n_actions])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceQModel(object):\n",
    "    def __init__(self, max_memory=500, discount=.9):\n",
    "        # Memory replay parameters\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "    def exp_remember(self, states):\n",
    "        self.memory.append(states.copy())\n",
    "        if len(self.memory) > self.max_memory:\n",
    "          del self.memory[0]\n",
    "\n",
    "    # based on https://gist.github.com/EderSantana/c7222daa328f0e885093\n",
    "    def exp_get_batch(self,batch_size=10):\n",
    "        len_memory = len(self.memory)\n",
    "        n_examples = min(len_memory, batch_size)\n",
    "        inputs = np.zeros((n_examples, n_input))\n",
    "        targets = np.zeros((n_examples, n_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,size=n_examples)):\n",
    "            #get_memory\n",
    "            states = self.memory[idx]\n",
    "            state_t = states['state_t']\n",
    "            state_tp1 = states['state_tp1']\n",
    "            action = states['action']\n",
    "\n",
    "            # input\n",
    "            inputs[i] = state_t.astype(np.float64)\n",
    "\n",
    "            # targets - not correcting those which are not taken\n",
    "            feed_dict = {x: states['state_t'].reshape(1,-1)}\n",
    "            targets[i] = sess.run(pred, feed_dict)\n",
    "            \n",
    "            # acted action\n",
    "            feed_dict = {x: states['state_tp1'].reshape(1,-1)}\n",
    "            Qsa = np.max(sess.run(pred, feed_dict))\n",
    "\n",
    "            # check if endgame and if not apply discount\n",
    "            if states['endgame']:\n",
    "                targets[i,action] = states['reward'] # assign just reward if endgame\n",
    "            else:\n",
    "                targets[i,action] = states['reward'] + self.discount * Qsa\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create model\n",
    "def multilayer_perceptron(x, weights, biases):\n",
    "    # Hidden layer with RELU activation\n",
    "    layer_1 = tf.add(tf.matmul(x, weights['h1']), biases['b1'])\n",
    "    layer_1 = tf.nn.relu(layer_1)\n",
    "    \n",
    "    # Hidden layer with RELU activation\n",
    "#     layer_2 = tf.add(tf.matmul(layer_1, weights['h2']), biases['b2'])\n",
    "#     layer_2 = tf.nn.relu(layer_2)\n",
    "    \n",
    "    # Output layer with linear activation\n",
    "    out_layer = tf.matmul(layer_1, weights['out']) + biases['out']\n",
    "    return out_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'h1': tf.Variable(tf.random_normal([n_input, n_hidden_1],dtype=tf.float64)),\n",
    "    'h2': tf.Variable(tf.random_normal([n_hidden_1, n_hidden_2],dtype=tf.float64)),\n",
    "    'out': tf.Variable(tf.random_normal([n_hidden_2, n_actions],dtype=tf.float64))\n",
    "}\n",
    "biases = {\n",
    "    'b1': tf.Variable(tf.random_normal([n_hidden_1],dtype=tf.float64)),\n",
    "    'b2': tf.Variable(tf.random_normal([n_hidden_2],dtype=tf.float64)),\n",
    "    'out': tf.Variable(tf.random_normal([n_actions],dtype=tf.float64))\n",
    "}\n",
    "\n",
    "# Construct model\n",
    "pred = multilayer_perceptron(x, weights, biases)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_sum(tf.pow(pred-y, 2))/(2*batch_size)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0: lost after 8, cost 0.00664173055166\n",
      "1: lost after 10, cost 203.728626842\n",
      "2: lost after 10, cost 458.134591374\n",
      "3: lost after 10, cost 451.339088095\n",
      "4: lost after 9, cost 423.840632084\n",
      "5: lost after 16, cost 1016.84752259\n",
      "6: lost after 11, cost 703.262416235\n",
      "7: lost after 11, cost 1120.05682108\n",
      "8: lost after 17, cost 1825.55477612\n",
      "9: lost after 16, cost 1506.22951841\n",
      "10: lost after 54, cost 2948.82887553\n",
      "11: lost after 37, cost 2032.47233439\n",
      "12: lost after 40, cost 1832.3635203\n",
      "13: lost after 25, cost 1021.27352212\n",
      "14: lost after 8, cost 315.548843122\n",
      "15: lost after 9, cost 475.111502514\n",
      "16: lost after 10, cost 483.002848034\n",
      "17: lost after 11, cost 522.344383404\n",
      "18: lost after 9, cost 366.991523718\n",
      "19: lost after 9, cost 572.539159817\n",
      "20: lost after 10, cost 567.788158299\n",
      "21: lost after 9, cost 483.655001331\n",
      "22: lost after 9, cost 627.524978933\n",
      "23: lost after 8, cost 483.962250243\n",
      "24: lost after 8, cost 470.353657006\n",
      "25: lost after 10, cost 547.51917577\n",
      "26: lost after 8, cost 524.722236407\n",
      "27: lost after 10, cost 687.815572195\n",
      "28: lost after 12, cost 681.645515132\n",
      "29: lost after 62, cost 4060.79518833\n",
      "30: lost after 62, cost 3340.49177342\n",
      "31: lost after 52, cost 2636.21099769\n",
      "32: lost after 50, cost 2408.6126493\n",
      "33: lost after 46, cost 1913.28137849\n",
      "34: lost after 27, cost 1273.43664686\n",
      "35: lost after 10, cost 550.301194235\n",
      "36: lost after 60, cost 2501.49539803\n",
      "37: lost after 34, cost 1298.76038671\n",
      "38: lost after 58, cost 2264.30742805\n",
      "39: won!\n",
      "40: lost after 89, cost 2814.75661348\n",
      "41: lost after 72, cost 2280.9906299\n",
      "42: lost after 65, cost 1897.79628687\n",
      "43: lost after 86, cost 2392.40174106\n",
      "44: lost after 89, cost 2317.96695383\n",
      "45: lost after 96, cost 2335.34063476\n",
      "46: won!\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-ac9997c343e9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     45\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     46\u001b[0m         \u001b[1;31m# get experience replay\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 47\u001b[1;33m         \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexp_replay\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexp_get_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     48\u001b[0m         \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcost\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0my_batch\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     49\u001b[0m         \u001b[1;31m# Compute average loss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-3-0630d53f1bab>\u001b[0m in \u001b[0;36mexp_get_batch\u001b[1;34m(self, batch_size)\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[1;31m# targets - not correcting those which are not taken\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m             \u001b[0mfeed_dict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'state_t'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m             \u001b[0mtargets\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m             \u001b[1;31m# acted action\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    370\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    371\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 372\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    373\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    374\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    634\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    635\u001b[0m       results = self._do_run(handle, target_list, unique_fetches,\n\u001b[1;32m--> 636\u001b[1;33m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[0;32m    637\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    638\u001b[0m       \u001b[1;31m# The movers are no longer used. Delete them.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_run\u001b[1;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    706\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    707\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[1;32m--> 708\u001b[1;33m                            target_list, options, run_metadata)\n\u001b[0m\u001b[0;32m    709\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    710\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_do_call\u001b[1;34m(self, fn, *args)\u001b[0m\n\u001b[0;32m    713\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 715\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    716\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    717\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/python/client/session.pyc\u001b[0m in \u001b[0;36m_run_fn\u001b[1;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[0;32m    695\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[0;32m    696\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 697\u001b[1;33m                                  status, run_metadata)\n\u001b[0m\u001b[0;32m    698\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    699\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "sess = tf.Session()\n",
    "init = tf.initialize_all_variables()\n",
    "sess.run(init)\n",
    "\n",
    "# initialize states and experience replay\n",
    "states = {}\n",
    "exp_replay = ExperienceQModel(max_memory=max_memory)\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(training_epochs):\n",
    "    avg_cost = 0.\n",
    "    state_tp1 = env.reset()\n",
    "    done = False\n",
    "\n",
    "    for t in range(n_steps):\n",
    "        env.render()\n",
    "        state_t1 = np.array(state_tp1)\n",
    "        \n",
    "        # exploration cycle\n",
    "        if np.random.rand() <= exp_a-exp_b*epoch/100:\n",
    "            action = env.action_space.sample()\n",
    "        else:\n",
    "            feed_dict = {x: state_t1.reshape(1,-1)}\n",
    "            qvals = sess.run(pred, feed_dict)\n",
    "            action = np.argmax(qvals)\n",
    "\n",
    "        # take a next step\n",
    "        state_tp1, reward, done, info = env.step(action)\n",
    "\n",
    "        # print (reward,done)\n",
    "        # rewards\n",
    "        if (t == 99) and (done == False):\n",
    "            print(\"{}: won!\".format(epoch))\n",
    "        if done:\n",
    "            reward = 0;\n",
    "\n",
    "        # store experience\n",
    "        states['action'] = action\n",
    "        states['reward'] = float(reward)\n",
    "        states['endgame'] = done\n",
    "        states['state_t'] = np.array(state_t1)\n",
    "        states['state_tp1'] = np.array(state_tp1)\n",
    "        exp_replay.exp_remember(states)\n",
    "\n",
    "        # get experience replay\n",
    "        x_batch, y_batch = exp_replay.exp_get_batch(batch_size)\n",
    "        _, c = sess.run([optimizer, cost], feed_dict={x: x_batch, y: y_batch})\n",
    "        # Compute average loss\n",
    "        avg_cost += c / n_steps\n",
    "\n",
    "        # Lost\n",
    "        if done:\n",
    "            print(\"{}: lost after {}, cost {}\".format(epoch,t+1,avg_cost))\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "exp_replay.memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "feed_dict = {x: states['state_tp1'].reshape(1,-1)}\n",
    "qvals = sess.run(pred, feed_dict)\n",
    "print(qvals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "weights"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
