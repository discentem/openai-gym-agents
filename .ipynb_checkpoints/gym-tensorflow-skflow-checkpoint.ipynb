{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "import re\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class ExperienceQModel(object):\n",
    "    def __init__(self, env, monitor_file, log_dir, max_memory=10000, discount=.9, n_episodes=100, \n",
    "                 n_steps=100, batch_size=100, learning_rate = 0.01, dropout = 1.0,\n",
    "                 exploration=lambda x: 0.1, stop_training=10):\n",
    "        \n",
    "        # Memory replay parameters\n",
    "        self.max_memory = max_memory\n",
    "        self.memory = list()\n",
    "        self.discount = discount\n",
    "\n",
    "        # exploration\n",
    "        self.eps = exploration # epsilon-greedy as function of epoch\n",
    "        \n",
    "        # environment parameters\n",
    "        self.env = gym.make(env)\n",
    "        self.monitor_file = monitor_file\n",
    "        self.n_states = self.env.observation_space.shape[0]\n",
    "        self.n_actions = int(re.findall('\\d+',str(self.env.action_space))[0]) # shameless hack to get a dim of actions\n",
    "        \n",
    "        # training parameters\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_episodes = n_episodes\n",
    "        self.n_steps = n_steps # must be equal to episode length\n",
    "        self.batch_size = batch_size\n",
    "        self.stop_training = stop_training # stop training after stop_training consecutive wins\n",
    "        self.consec_wins = 0 # number of consecutive wins to stop training\n",
    "        self.global_step = 0 # global step\n",
    "\n",
    "        # Neural Network Parameters\n",
    "        self.n_hidden = [self.n_states]\n",
    "        \n",
    "#         # Initialize tensor flow parameters\n",
    "        self.x = tf.placeholder(tf.float32, [None, self.n_states],name='states')\n",
    "        self.y = tf.placeholder(tf.float32, [None, self.n_actions],name='qvals')\n",
    "#         self.keep_prob = dropout\n",
    "#         self.dropout = tf.placeholder(tf.float32,name='dropout')\n",
    "\n",
    "        # Tensorboard directory\n",
    "        try:\n",
    "            shutil.rmtree(log_dir) #clean\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # define model\n",
    "        self.estimator = tf.contrib.learn.DNNRegressor(\n",
    "            hidden_units=self.n_hidden,\n",
    "            optimizer=tf.train.AdamOptimizer(\n",
    "              learning_rate=self.learning_rate,\n",
    "        ))\n",
    "\n",
    "    # process reward\n",
    "    def exp_process_reward(self,ts,reward,endgame):\n",
    "        # if ts < self.n_steps-1 and endgame == True:\n",
    "        #     reward = -1.0*ts/5.0 #penalize last moves\n",
    "        # if ts == self.n_steps-1 and endgame == True:\n",
    "        #     reward = ts #win\n",
    "        # if endgame == True:\n",
    "            # reward = 1.0*ts\n",
    "        return reward\n",
    "\n",
    "    # saving to memory\n",
    "    def exp_save_to_memory(self, states):\n",
    "        self.memory.append(states.copy())\n",
    "        if len(self.memory) > self.max_memory:\n",
    "          del self.memory[0]\n",
    "\n",
    "    # based on https://gist.github.com/EderSantana/c7222daa328f0e885093\n",
    "    def exp_get_batch(self):\n",
    "        len_memory = len(self.memory)\n",
    "        n_examples = min(len_memory, self.batch_size)\n",
    "        inputs = np.zeros((n_examples, self.n_states))\n",
    "        targets = np.zeros((n_examples, self.n_actions))\n",
    "        for i, idx in enumerate(np.random.randint(0, len_memory,size=n_examples)):\n",
    "            #get_memory\n",
    "            states = self.memory[idx]\n",
    "\n",
    "            # input\n",
    "            inputs[i] = states['state_t'].astype(np.float32)\n",
    "\n",
    "            # targets - not correcting those which are not taken\n",
    "#             feed_dict = {self.x: states['state_t'].reshape(1,-1), self.dropout: 1.0}\n",
    "#             targets[i] = self.session.run(self.predictor, feed_dict)\n",
    "#             targets[i] = self.estimator.predict(x=states['state_t'].reshape(1,-1))\n",
    "            targets[i] = [1.2,2.4]\n",
    "            \n",
    "            # acted action\n",
    "#             feed_dict = {self.x: states['state_tp1'].reshape(1,-1), self.dropout: 1.0}\n",
    "#             Qsa = np.max(self.estimator.predict(x=states['state_tp1'].reshape(1,-1)))\n",
    "#             Qsa = np.max(self.session.run(self.predictor, feed_dict))\n",
    "            Qsa = 4.\n",
    "\n",
    "            # check if endgame and if not use Bellman's equation\n",
    "            if states['endgame']:\n",
    "                targets[i,states['action']] = states['reward']\n",
    "            else:\n",
    "                targets[i,states['action']] = states['reward'] + self.discount * Qsa\n",
    "        return inputs, targets\n",
    "\n",
    "    # Train loop\n",
    "    def tf_train_model(self):\n",
    "        # start open ai monitor\n",
    "        if self.monitor_file:\n",
    "            self.env.monitor.start(self.monitor_file,force=True)\n",
    "\n",
    "        # Training cycle\n",
    "        for epoch in range(self.n_episodes):\n",
    "\n",
    "            # restart episode\n",
    "            state_tp1 = self.env.reset()\n",
    "            endgame = False\n",
    "            avg_loss = 0.\n",
    "            avg_max_qval = 0.\n",
    "            states = {}\n",
    "\n",
    "            for t in range(self.n_steps):\n",
    "                self.env.render()\n",
    "                state_t1 = np.array(state_tp1)\n",
    "        \n",
    "                # epsilon-greedy exploration\n",
    "                if self.consec_wins < self.stop_training and np.random.rand() <= self.eps(epoch):\n",
    "                    action = self.env.action_space.sample()\n",
    "                else:\n",
    "#                     feed_dict = {self.x: state_t1.reshape(1,-1), self.dropout: 1.0}\n",
    "                    qvals = self.estimator.predict(x=state_t1.reshape(1,-1))\n",
    "#                     qvals = self.session.run(self.predictor, feed_dict)\n",
    "                    avg_max_qval += np.max(qvals)\n",
    "                    action = np.argmax(qvals)\n",
    "\n",
    "                # take a next step\n",
    "                state_tp1, reward, endgame, info = self.env.step(action)\n",
    "\n",
    "                # process reward\n",
    "                reward = self.exp_process_reward(t,reward,endgame)\n",
    "\n",
    "                #store experience\n",
    "                states['action'] = action\n",
    "                states['reward'] = float(reward)/10\n",
    "                states['endgame'] = endgame\n",
    "                states['state_t'] = np.array(state_t1)\n",
    "                states['state_tp1'] = np.array(state_tp1)\n",
    "                self.exp_save_to_memory(states)\n",
    "\n",
    "                # Training loop\n",
    "                if self.consec_wins < self.stop_training:\n",
    "                    # get experience replay\n",
    "                    x_batch, y_batch = self.exp_get_batch()\n",
    "                    # create feed dictionary\n",
    "#                     feed_dict = {self.x: x_batch, self.y: y_batch, self.dropout: self.keep_prob}\n",
    "                    # training\n",
    "                    self.estimator.fit(x=x_batch,y=y_batch)\n",
    "#                     _, loss, summary = self.session.run([self.train_op, self.loss, self.merged_summary_op],\n",
    "#                         feed_dict=feed_dict)\n",
    "                    \n",
    "                    # add summary to the summary_writer\n",
    "#                     self.global_step += x_batch.shape[0]\n",
    "#                     self.summary_writer.add_summary(summary,self.global_step)\n",
    "                    # avg loss\n",
    "                    avg_loss += loss\n",
    "\n",
    "                # Check if lost or not\n",
    "                if endgame == True:\n",
    "                    if (t == self.n_steps-1):\n",
    "                        self.consec_wins +=1\n",
    "                        print(\"{:4d}: won!\".format(epoch+1))\n",
    "                        break\n",
    "                    else:\n",
    "                        self.consec_wins = 0\n",
    "                        print(\"{:4d}: lost after {:3d}, cost {:8.4f}, qval {:8.4f}\".\n",
    "                                format(epoch+1,t+1,avg_loss/t,avg_max_qval/t))\n",
    "                        break\n",
    "\n",
    "        # close monitor session\n",
    "        if self.monitor_file:\n",
    "            self.env.monitor.close()\n",
    "\n",
    "    # submit result for the leaderboard\n",
    "    def submit_result(self,algo_id,api_key):\n",
    "        gym.upload(self.monitor_file,\n",
    "            algorithm_id=algo_id,\n",
    "            api_key=api_key,\n",
    "            ignore_open_monitors=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2016-07-11 19:13:57,154] Making new env: CartPole-v0\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "Couldn't find trained model at /tmp/tmphwAHBN.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m\u001b[0m",
      "\u001b[1;31mNotFittedError\u001b[0mTraceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-2884f2a348c8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m )\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtf_train_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-32-019c1e2ea429>\u001b[0m in \u001b[0;36mtf_train_model\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    120\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m \u001b[1;31m#                     feed_dict = {self.x: state_t1.reshape(1,-1), self.dropout: 1.0}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                     \u001b[0mqvals\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mstate_t1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;31m#                     qvals = self.session.run(self.predictor, feed_dict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m                     \u001b[0mavg_max_qval\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mqvals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/dnn_linear_combined.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, input_fn, batch_size)\u001b[0m\n\u001b[0;32m    626\u001b[0m     \"\"\"\n\u001b[0;32m    627\u001b[0m     return super(DNNLinearCombinedRegressor, self).predict(\n\u001b[1;32m--> 628\u001b[1;33m         x=x, input_fn=input_fn, batch_size=batch_size)\n\u001b[0m\u001b[0;32m    629\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    630\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_loss_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, input_fn, batch_size, outputs)\u001b[0m\n\u001b[0;32m    308\u001b[0m       \u001b[0minput_fn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_fn\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_get_predict_input_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    309\u001b[0m     return self._infer_model(input_fn=input_fn, feed_fn=feed_fn,\n\u001b[1;32m--> 310\u001b[1;33m                              outputs=outputs)\n\u001b[0m\u001b[0;32m    311\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    312\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_variable_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/local/lib/python2.7/dist-packages/tensorflow/contrib/learn/python/learn/estimators/estimator.pyc\u001b[0m in \u001b[0;36m_infer_model\u001b[1;34m(self, input_fn, feed_fn, outputs)\u001b[0m\n\u001b[0;32m    558\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheckpoint_path\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    559\u001b[0m       raise NotFittedError(\"Couldn't find trained model at %s.\"\n\u001b[1;32m--> 560\u001b[1;33m                            % self._model_dir)\n\u001b[0m\u001b[0;32m    561\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    562\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNotFittedError\u001b[0m: Couldn't find trained model at /tmp/tmphwAHBN."
     ]
    }
   ],
   "source": [
    "model = ExperienceQModel(\n",
    "    env='CartPole-v0',\\\n",
    "    monitor_file = None,\\\n",
    "    log_dir = '/tmp/tf/cartpole-256_1.e-3',\\\n",
    "    max_memory=40000,\\\n",
    "    discount=.90,\\\n",
    "    n_episodes=400,\\\n",
    "    n_steps=200,\\\n",
    "    batch_size=256,\\\n",
    "    learning_rate = 1.e-3,\\\n",
    "    dropout = 1.0,\\\n",
    "    exploration = lambda x: 0.1 if x<50 else 0,\\\n",
    "    stop_training = 10\n",
    ")\n",
    "\n",
    "model.tf_train_model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
